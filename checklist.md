# Checklist de Implementa√ß√£o - Stack Fase 1

Este checklist organiza as tarefas necess√°rias para aderir completamente √† stack definida em `stack.md`, seguindo uma abordagem **incremental e test√°vel** em cada etapa.

---

## Status Geral

- **Infraestrutura**: Docker Compose configurado (PostgreSQL na porta 5433, Redis, MinIO)
- **Depend√™ncias**: Bibliotecas instaladas (FastAPI, SQLModel, Arq, psycopg2-binary, etc.)
- **Endpoint b√°sico**: `/health` funcionando
- **Etapa 1**: ‚úÖ Conclu√≠da - Modelos Tenant, User, Job criados e migrados
- **Implementa√ß√£o**: ~25% - Funda√ß√µes do banco de dados e modelos b√°sicos implementados

---

## Caminho M√≠nimo Incremental

Cada etapa abaixo entrega algo **vis√≠vel e test√°vel** via Swagger (`/docs`) ou curl, sem quebrar o que j√° funciona.

### Etapa 0: Base (J√° feito)
- [x] Docker Compose sobe sem erros
- [x] `/health` retorna `{"status": "ok"}`
- [x] Depend√™ncias instaladas

### Etapa 1: DB + 3 tabelas b√°sicas
- [x] Modelos: Tenant, User, Job
- [x] Alembic configurado e migra√ß√£o aplicada
- [x] Endpoint `POST /tenants` (criar tenant simples)
- [x] Testar: criar tenant via `/docs`, verificar no banco

### Etapa 2: OAuth + JWT + `/me`
- [ ] OAuth Google integrado
- [ ] JWT com `tenant_id` no token
- [ ] Endpoint `GET /me` retorna User do banco
- [ ] Testar: login via Google, verificar JWT, chamar `/me`

### Etapa 3: Upload + File + MinIO
- [ ] Modelo File
- [ ] StorageService b√°sico (upload/download)
- [ ] Endpoint `POST /files/upload` retorna URL/presigned
- [ ] Testar: upload arquivo, verificar MinIO e banco

### Etapa 4: Arq - Job fake primeiro
- [ ] WorkerSettings configurado
- [ ] Job `PING_JOB` (fake, s√≥ valida fila)
- [ ] Endpoint `POST /jobs/ping` cria Job e enfileira
- [ ] Testar: criar job, ver worker processar, ver status

### Etapa 5: Arq - EXTRACT_DEMANDS
- [ ] Job `EXTRACT_DEMANDS` com OpenAI (adaptar `demand/read.py`)
- [ ] Salvar resultado como JSON no `Job.result_data`
- [ ] Endpoint `POST /jobs/extract` (recebe file_id)
- [ ] Testar: upload ‚Üí extract ‚Üí ver resultado no Job

### Etapa 6: ScheduleVersion + GenerateSchedule
- [ ] Modelo ScheduleVersion
- [ ] Job `GENERATE_SCHEDULE` (usar c√≥digo de `strategy/`)
- [ ] Salvar resultado no ScheduleVersion
- [ ] Endpoint `POST /schedules/generate`
- [ ] Testar: gerar escala, ver ScheduleVersion criado

### Etapa 7: PDF + Publica√ß√£o
- [ ] Gerar PDF (adaptar `output/day.py`)
- [ ] Upload PDF para S3
- [ ] Endpoint `POST /schedules/{id}/publish`
- [ ] Endpoint `GET /schedules/{id}/pdf` (download)
- [ ] Testar: gerar ‚Üí publicar ‚Üí download PDF

---

## FASE 1: Funda√ß√µes - Modelos e Banco de Dados

### 1.1 Modelos SQLModel (M√≠nimo Inicial: 5 tabelas)

**Come√ßar simples, evoluir depois:**

- [x] Criar `app/models/__init__.py`
- [x] Criar `app/models/base.py`:
  - [x] Classe base `BaseModel` (SQLModel) com:
    - [x] `id: int` (primary key)
    - [x] `created_at: datetime`
    - [x] `updated_at: datetime`
    - [ ] `tenant_id: int` (ForeignKey para Tenant, nullable=False) - *Nota: BaseModel n√£o tem tenant_id, apenas modelos filhos*
- [x] Criar `app/models/tenant.py`:
  - [x] Modelo `Tenant` (id, name, slug, created_at, updated_at)
  - [x] Sem `tenant_id` (√© a raiz do multi-tenant)
- [x] Criar `app/models/user.py`:
  - [x] Modelo `User` (id, email, name, role, tenant_id FK, auth_provider, created_at, updated_at)
  - [x] √çndice √∫nico em `(email, tenant_id)`
- [x] Criar `app/models/job.py`:
  - [x] Modelo `Job` (id, tenant_id, job_type, status, input_data JSON, result_data JSON, error_message, created_at, updated_at, completed_at)
  - [x] Enum para `job_type`: `PING`, `EXTRACT_DEMANDS`, `GENERATE_SCHEDULE`
  - [x] Enum para `status`: `PENDING`, `RUNNING`, `COMPLETED`, `FAILED`
  - [x] **Nota**: `result_data` guarda Demandas como JSON inicialmente
- [ ] Criar `app/models/file.py`:
  - [ ] Modelo `File` (id, tenant_id, filename, content_type, s3_key, s3_url, file_size, uploaded_at, created_at)
- [ ] Criar `app/models/schedule_version.py`:
  - [ ] Modelo `ScheduleVersion` (id, tenant_id, name, period_start, period_end, status, version_number, job_id FK nullable, pdf_file_id FK nullable, result_data JSON, generated_at, published_at, created_at)
  - [ ] Enum para `status`: `DRAFT`, `PUBLISHED`, `ARCHIVED`
  - [ ] **Nota**: `result_data` guarda resultado da gera√ß√£o (aloca√ß√£o) como JSON

**Evolu√ß√£o futura (quando necess√°rio):**
- [ ] Criar `app/models/schedule.py` (quando precisar de m√∫ltiplas vers√µes por schedule)
- [ ] Criar `app/models/demand.py` (quando precisar queryar demandas diretamente)
- [ ] Criar `app/models/professional.py` (quando precisar CRUD de profissionais)

### 1.2 Configura√ß√£o do Alembic
- [x] Atualizar `alembic/env.py`:
  - [x] Importar `Base` do SQLModel (ou metadata do SQLAlchemy)
  - [x] Definir `target_metadata` apontando para os modelos
  - [x] Garantir que `compare_type=True` est√° ativo
- [x] Criar migra√ß√£o inicial: `alembic revision --autogenerate -m "Initial schema - Tenant, User, Job"`
- [x] Revisar migra√ß√£o gerada (verificar se 3 tabelas foram inclu√≠das)
- [x] Testar migra√ß√£o: `alembic upgrade head`
- [x] Verificar se tabelas foram criadas no PostgreSQL

### 1.3 Utilit√°rios de Banco
- [x] Criar `app/db/__init__.py`
- [x] Criar `app/db/session.py`:
  - [x] Fun√ß√£o `get_session()` (dependency do FastAPI)
  - [x] Configurar engine do SQLModel com `DATABASE_URL`
  - [x] Criar engine singleton
- [x] Criar `app/db/base.py`:
  - [x] Fun√ß√£o para criar todas as tabelas (√∫til para testes)

---

## FASE 2: Autentica√ß√£o e Multi-Tenant

### 2.1 Integra√ß√£o de Autentica√ß√£o
- [ ] Criar `app/auth/__init__.py`
- [ ] Criar `app/auth/jwt.py`:
  - [ ] Fun√ß√£o `create_access_token(user_id, tenant_id, role)` retornando JWT
  - [ ] Fun√ß√£o `verify_token(token)` retornando payload (user_id, tenant_id, role)
  - [ ] Usar `JWT_SECRET` e `JWT_ISSUER` do ambiente
  - [ ] Claims obrigat√≥rios: `user_id`, `tenant_id`, `role`, `exp`, `iat`, `iss`
- [ ] Criar `app/auth/dependencies.py`:
  - [ ] Dependency `get_current_user(session, token)` retornando User
  - [ ] Dependency `require_role(role: str)` para verificar permiss√µes
  - [ ] Dependency `get_current_tenant(session, token)` retornando Tenant
- [ ] Migrar l√≥gica do `login.py` para `app/auth/oauth.py`:
  - [ ] Fun√ß√£o `verify_google_token(token)`
  - [ ] Endpoint `POST /auth/google` (adaptar do login.py)
  - [ ] Endpoint `POST /auth/google/register` (adaptar do login.py)
  - [ ] Integrar com modelos User/Tenant (criar usu√°rio no banco, n√£o JSON)
- [ ] Atualizar `app/api/routes.py`:
  - [ ] Importar router de autentica√ß√£o
  - [ ] Incluir rotas de auth
- [ ] Testar autentica√ß√£o:
  - [ ] Login com Google retorna JWT v√°lido
  - [ ] JWT cont√©m `tenant_id`
  - [ ] `GET /me` retorna dados do usu√°rio do banco

### 2.2 Multi-Tenant Enforcement
- [ ] Criar `app/services/tenant_service.py`:
  - [ ] Fun√ß√£o `get_tenant_by_id(tenant_id)`
  - [ ] Fun√ß√£o `create_tenant(name, slug)`
- [ ] Criar `app/middleware/tenant.py`:
  - [ ] Middleware que extrai `tenant_id` do JWT e adiciona ao `request.state`
  - [ ] Validar que tenant existe no banco
- [ ] Aplicar middleware em `app/main.py`
- [ ] Criar helper `get_tenant_id(request)` para endpoints
- [ ] Documentar padr√£o: todas as queries devem usar `tenant_id` do `request.state`

---

## FASE 3: Storage (S3/MinIO)

### 3.1 Configura√ß√£o S3/MinIO
- [ ] Criar `app/storage/__init__.py`
- [ ] Criar `app/storage/config.py`:
  - [ ] Classe `S3Config` lendo vari√°veis: `S3_ENDPOINT_URL`, `S3_ACCESS_KEY_ID`, `S3_SECRET_ACCESS_KEY`, `S3_BUCKET_NAME`, `S3_REGION`, `S3_USE_SSL`
- [ ] Criar `app/storage/client.py`:
  - [ ] Classe `S3Client` usando boto3
  - [ ] M√©todo `upload_file(file_path, s3_key, content_type) -> s3_url`
  - [ ] M√©todo `download_file(s3_key, local_path)`
  - [ ] M√©todo `get_presigned_url(s3_key, expiration)`
  - [ ] M√©todo `ensure_bucket_exists()` (criar bucket se n√£o existir)
- [ ] Criar `app/storage/service.py`:
  - [ ] Classe `StorageService` que usa `S3Client`
  - [ ] M√©todo `upload_imported_file(tenant_id, file, filename) -> File model`
  - [ ] M√©todo `upload_schedule_pdf(tenant_id, schedule_version_id, pdf_bytes) -> File model`
  - [ ] M√©todo `get_file_url(file_id, tenant_id) -> str`
  - [ ] Padr√£o de S3 keys: `{tenant_id}/{file_type}/{filename}`

### 3.2 Integra√ß√£o com Modelos
- [ ] Criar endpoint `POST /files/upload`:
  - [ ] Receber arquivo via multipart
  - [ ] Upload para S3 (StorageService)
  - [ ] Criar File no banco
  - [ ] Retornar `{file_id, s3_url, presigned_url}`
- [ ] Testar upload/download:
  - [ ] Upload de arquivo cria registro no banco e arquivo no MinIO
  - [ ] Download retorna arquivo correto
  - [ ] URLs presignadas funcionam

---

## FASE 4: Jobs Ass√≠ncronos (Arq) - Incremental

### 4.1 Configura√ß√£o B√°sica de Workers
- [ ] Atualizar `app/workers/worker_settings.py`:
  - [ ] Classe `WorkerSettings` (herdando de `arq.worker.WorkerSettings`)
  - [ ] Configurar `redis_settings` usando `REDIS_URL`
  - [ ] Configurar `max_jobs=10` (inicial)
- [ ] Atualizar `app/workers/run.py`:
  - [ ] Usar `WorkerSettings` corretamente
  - [ ] Iniciar worker com `arq.run_worker(WorkerSettings)`

### 4.2 Job Fake (PING) - Validar Fila
- [ ] Criar `app/jobs/__init__.py`
- [ ] Criar `app/jobs/ping.py`:
  - [ ] Fun√ß√£o `ping_job(ctx, job_id, tenant_id, message)` decorada com `@arq.job`
  - [ ] L√≥gica simples: atualizar Job com `result_data={"message": message}`
- [ ] Criar endpoint `POST /jobs/ping`:
  - [ ] Criar Job no banco (tipo PING, status PENDING)
  - [ ] Enfileirar job no Arq
  - [ ] Retornar `{job_id}`
- [ ] Criar endpoint `GET /jobs/{job_id}`:
  - [ ] Retornar status e resultado do Job
- [ ] Testar: criar job ping, ver worker processar, verificar status COMPLETED

### 4.3 Job EXTRACT_DEMANDS (OpenAI)
- [ ] Criar `app/jobs/extract_demands.py`:
  - [ ] Fun√ß√£o `extract_demands_job(ctx, job_id, file_id, tenant_id)` decorada com `@arq.job`
  - [ ] L√≥gica:
    1. Buscar File do banco (validar tenant_id)
    2. Download do S3 para arquivo tempor√°rio
    3. Chamar `extract_demands_from_file()` (adaptar `demand/read.py`)
    4. Salvar resultado como JSON no `Job.result_data`
    5. Atualizar Job status (COMPLETED/FAILED)
- [ ] Criar `app/ai/openai_provider.py`:
  - [ ] Fun√ß√£o `extract_demands_from_file(file_path, file_type) -> List[dict]`
  - [ ] Adaptar c√≥digo de `demand/read.py` (OpenAI Vision)
  - [ ] Retornar lista de demandas como dicts
- [ ] Criar endpoint `POST /jobs/extract`:
  - [ ] Receber `file_id`
  - [ ] Criar Job (tipo EXTRACT_DEMANDS, status PENDING)
  - [ ] Enfileirar job no Arq
  - [ ] Retornar `{job_id}`
- [ ] Testar: upload arquivo ‚Üí extract ‚Üí ver demandas no `Job.result_data`

### 4.4 Job GENERATE_SCHEDULE
- [ ] Criar `app/jobs/generate_schedule.py`:
  - [ ] Fun√ß√£o `generate_schedule_job(ctx, job_id, schedule_version_id, tenant_id, allocation_mode)` decorada com `@arq.job`
  - [ ] L√≥gica:
    1. Buscar ScheduleVersion do banco (validar tenant_id)
    2. Buscar demandas do `Job.result_data` (do job de extra√ß√£o anterior)
    3. Buscar profissionais (por enquanto, usar dados mock ou JSON)
    4. Chamar solver (greedy ou CP-SAT) - usar c√≥digo de `strategy/`
    5. Salvar resultado no `ScheduleVersion.result_data`
    6. Gerar PDF (usar c√≥digo de `output/day.py`)
    7. Upload PDF para S3
    8. Atualizar `ScheduleVersion.pdf_file_id`
    9. Atualizar Job status
- [ ] Criar endpoint `POST /schedules/generate`:
  - [ ] Receber `schedule_version_id`, `allocation_mode`
  - [ ] Criar Job (tipo GENERATE_SCHEDULE, status PENDING)
  - [ ] Enfileirar job no Arq
  - [ ] Retornar `{job_id}`
- [ ] Testar: gerar escala, ver ScheduleVersion criado, ver PDF no S3

**Nota**: Abstra√ß√£o completa de AI Provider (interface formal) fica para depois, quando precisar plugar outro provedor.

---

## FASE 5: API Endpoints Completos

### 5.1 Endpoints de Tenants
- [ ] Criar `app/api/tenants.py`:
  - [ ] `POST /tenants` (criar tenant - apenas admin ou primeiro usu√°rio)
  - [ ] `GET /tenants/me` (tenant atual do usu√°rio)

### 5.2 Endpoints de Schedules
- [ ] Criar `app/api/schedules.py`:
  - [ ] `GET /schedules` (listar ScheduleVersions - filtrado por tenant)
  - [ ] `POST /schedules` (criar ScheduleVersion - filtrado por tenant)
  - [ ] `GET /schedules/{id}` (detalhes - validar tenant)
  - [ ] `POST /schedules/{id}/publish` (publicar vers√£o - validar tenant)
  - [ ] `GET /schedules/{id}/pdf` (download PDF - validar tenant)
  - [ ] Retornar URL presignada do S3

### 5.3 Endpoints de Jobs
- [ ] Atualizar `app/api/jobs.py`:
  - [ ] `GET /jobs` (listar jobs do tenant)
  - [ ] `GET /jobs/{job_id}` (detalhes - validar tenant)

### 5.4 Valida√ß√µes e Seguran√ßa
- [ ] Garantir que TODOS os endpoints validam tenant_id:
  - [ ] Extrair de JWT
  - [ ] Validar que tenant existe
  - [ ] Filtrar queries por tenant_id
- [ ] Garantir que endpoints de cria√ß√£o/atualiza√ß√£o n√£o permitem alterar tenant_id
- [ ] Documentar API com OpenAPI/Swagger (FastAPI j√° faz isso)

---

## FASE 6: Integra√ß√£o de C√≥digo Existente

### 6.1 Adapta√ß√£o de Solvers
- [ ] Revisar `strategy/greedy/solve.py`:
  - [ ] Adaptar para receber demandas como List[dict] (do JSON)
  - [ ] Adaptar para receber profissionais como List[dict]
  - [ ] Retornar resultado como dict (compat√≠vel com ScheduleVersion.result_data)
- [ ] Revisar `strategy/cd_sat/solve.py`:
  - [ ] Mesma adapta√ß√£o acima
- [ ] Criar `app/services/schedule_service.py`:
  - [ ] Fun√ß√£o `generate_schedule(demands, professionals, allocation_mode) -> dict`
  - [ ] Chama solver apropriado (greedy ou CP-SAT)

### 6.2 Adapta√ß√£o de Gera√ß√£o de PDF
- [ ] Revisar `output/day.py`:
  - [ ] Adaptar `render_pdf()` para receber dados do ScheduleVersion
  - [ ] Retornar bytes do PDF (n√£o salvar em arquivo)
- [ ] Integrar no job `generate_schedule_job`:
  - [ ] Gerar PDF em mem√≥ria
  - [ ] Upload para S3 via StorageService

### 6.3 Manuten√ß√£o de Compatibilidade
- [ ] Manter `app.py` funcionando (n√£o quebrar c√≥digo legado)
- [ ] Criar script de migra√ß√£o de dados se necess√°rio:
  - [ ] Criar tenant padr√£o
  - [ ] Associar usu√°rios existentes a tenant

---

## FASE 7: Testes e Valida√ß√£o

### 7.1 Testes B√°sicos
- [ ] Testar fluxo completo via `/docs`:
  1. Criar tenant
  2. Login (obter JWT)
  3. Upload arquivo
  4. Job de extra√ß√£o processa
  5. Criar ScheduleVersion
  6. Job de gera√ß√£o processa
  7. Publicar escala
  8. Download PDF
- [ ] Testar multi-tenant isolation (usu√°rio de tenant A n√£o v√™ dados de tenant B)
- [ ] Testar que jobs respeitam tenant_id

### 7.2 Valida√ß√£o de Princ√≠pios
- [ ] Princ√≠pio 1: Requests HTTP nunca rodam solver/IA (sempre criam Job)
- [ ] Princ√≠pio 2: ScheduleVersion imut√°vel, publica√ß√£o separada
- [ ] Princ√≠pio 3: Multi-tenant por tenant_id em todas as tabelas
- [ ] Princ√≠pio 4: Storage fora do banco (S3, banco s√≥ metadados)

---

## FASE 8: Frontend e Mobile (Futuro)

### 8.1 Frontend Web (Next.js)
- [ ] Criar projeto Next.js
- [ ] Configurar autentica√ß√£o (OAuth Google)
- [ ] P√°ginas: Login, Dashboard, Importa√ß√£o, Escalas
- [ ] Integra√ß√£o com API

### 8.2 Mobile (React Native)
- [ ] Criar projeto React Native
- [ ] Configurar autentica√ß√£o (OAuth Google)
- [ ] Telas: Login, Lista de Escalas, Detalhes de Escala
- [ ] Integra√ß√£o com API

---

## üìù Notas de Implementa√ß√£o

### Filosofia: M√≠nimo Test√°vel
- Cada etapa entrega algo **vis√≠vel e test√°vel**
- Testar via Swagger (`/docs`) ou curl antes de avan√ßar
- N√£o criar abstra√ß√µes antes da hora (ex: AI Provider interface completa)
- Evoluir dom√≠nio quando realmente precisar (ex: Demand como tabela)

### Ordem de Prioridade
1. **Cr√≠tico**: Fases 1-4 (funda√ß√µes, auth, storage, jobs b√°sicos)
2. **Importante**: Fase 5 (API endpoints)
3. **Necess√°rio**: Fase 6 (integra√ß√£o)
4. **Desej√°vel**: Fase 7 (testes)
5. **Futuro**: Fase 8 (frontend/mobile)

### Boas Pr√°ticas
- Sempre validar `tenant_id` em queries
- Sempre criar Job antes de enfileirar
- Sempre usar StorageService para arquivos (nunca salvar no banco)
- Manter c√≥digo legado funcionando durante migra√ß√£o
- Commits pequenos e frequentes
- Testar cada etapa antes de avan√ßar

### Pontos de Aten√ß√£o
- N√£o quebrar `app.py` (c√≥digo legado ainda pode ser usado)
- Performance: jobs ass√≠ncronos s√£o essenciais (solver pode demorar)
- Seguran√ßa: validar tenant_id em TODOS os endpoints
- Storage: MinIO em dev, S3 real em produ√ß√£o (configurar via env)
- Sa√≠da: apenas PDF (n√£o Excel/CSV)

### Evolu√ß√£o Futura (Quando Necess√°rio)
- [ ] Promover Demand de JSON para tabela (quando precisar queryar diretamente)
- [ ] Criar modelo Schedule (quando precisar m√∫ltiplas vers√µes por schedule)
- [ ] Criar modelo Professional (quando precisar CRUD de profissionais)
- [ ] Abstra√ß√£o completa de AI Provider (quando precisar plugar outro provedor)
- [ ] Endpoints mobile espec√≠ficos (quando criar app React Native)

---

## Checklist de Valida√ß√£o Final

Antes de considerar completo, verificar:

- [x] 3 modelos SQLModel criados e migrados (Tenant, User, Job) - *Fase 1 conclu√≠da*
- [ ] Modelos File e ScheduleVersion (pr√≥ximas etapas)
- [ ] Autentica√ß√£o funcionando com tenant_id no JWT
- [ ] Multi-tenant enforcement ativo em todos os endpoints
- [ ] Storage S3/MinIO funcionando (upload/download)
- [ ] Jobs Arq processando corretamente (PING, EXTRACT, GENERATE)
- [ ] API endpoints seguindo princ√≠pios arquiteturais
- [ ] C√≥digo legado ainda funciona (ou foi migrado)
- [ ] Docker Compose sobe sem erros
- [ ] Migra√ß√µes Alembic aplicam sem erros
- [ ] Fluxo completo test√°vel via `/docs`

---

**√öltima atualiza√ß√£o**: Refatorado para abordagem incremental e test√°vel.
